[Joscha: Computational Meta-Psychology](https://www.youtube.com/watch?v=WRdJCFEqFTU)

[0:37](https://youtu.be/WRdJCFEqFTU?t=37)

I'm interested in understanding how the mind works. And I believe the most brutal perspective of looking at minds is to understand that we are systmes that if you saw patterns at them, then you find meaning. We find meaning in those very particular ways and this is what makes us who we are.
The way to study and understand who we are, in my understanding is to build models of the information processing that constitutes our minds.

[1:01](https://youtu.be/WRdJCFEqFTU?t=61)

Last year about the same time. I answered the four big questions about philosophy.

- What's the nature of reality
- What can we know?
- Who are we?
- What should we do?

How can I top this?

I'm gonna give you. The "drama that divided the planet".
What color is the dress.
I mean, if you do not have any mental defects. You can clearly see It's in white and gold, right?
Turns out, most people seems to have mental defects, say it's in blue and black. I've no idea why.
Well, okie, I have an idea why that is the case. I guess you've got too has to do with color renormalization, color renormalization happens differently apparently in different people. We have differnt wireing to renormalize the white balance. And it seems to be working in real world situations in pretty much the same way. 

But not necessarily for photographs, which  have only very small fringe around them which gives you hints about the lighting situation. That's why you get this huge divergence which is amazaing.

[2:27](https://youtu.be/WRdJCFEqFTU?t=147)
So what we see that our minds cannot know objective truth in anyway outside of mathematics. They can generate meaning though. 

How does this work?
I did robotic soccer for a while. And there you have the situation that you have a bunch of robot that are situated on the playing field.
And they have a model of what goes on in the playing field. And physics generate data for their sensors. They read inputs of their sensors. And they use them to update their world model.

And sometimes we didn't want to take the whole playing field along. And the physical robot they are expensive and heavy and so on.
If you just want to improve the learning and the game play of the robot. You can use a simulation. 
So we wrote a computer simulation of the playing field and physics and so on. They generate pretty much the same data. And put the robot mind into a simulated robot body.
And it works just as well, that is if you are the robot, you cannot know the difference if you are the robot. You cannot know what's out there. The only thing you get to see is what is the structure of the data at your systemic interface. And then you can derive model from this.
This is pretty much the situation we are in. We are minds that are somehow computational. They are able to find regularity and patterns. And we seem to have access to something that is full of regularity, so we can make sense of it.
Now if you discover, that you are in same situation of these robots. Basically you discover you are some kind of aparently biological robot that doesn't have direct access to the world of concepts, has never actually seen matter and energy and other people. All it got to see was little bits of information that was transmitted through nerves and brain have to make sense of them, by counting them in elaborate ways.

What's the best model of the world. What's the really the state of affairs? What's the system that you are in? And what's the algorithm you should be using to fix your world model?
And this question is pretty old. And I there has been answers for the first time by Ray Solomonoff in the 1960s.
He discovered an algorithm that you can apply when you discover you are a robot, all you got is data. What is the world like?
This algorithm is basically a combination of Bayesian reasoning + Induction + Occam's Razor.
We can mathematically prove that we cannot do better than Solomonoff induction.
Unfortunately, Solomonoff induction is not quite computable.
But everything we are going to do going to be some approximation of Solomonoff induction.
So our concepts cannot really refer to "facts in the world" out there.
We do not get the truth by refering to stuff out there in the world.
We get "meaning" by "suitable encoding" patterns at our systematic interface.

[5:08](https://youtu.be/WRdJCFEqFTU?t=308)

AI has recently made huge progress in encoding data and perceptual interfaces.